version: '3.8'

services:
  airflow:
    build:
      context: ./airflow
    volumes:
      - ./airflow/dags:/usr/local/airflow/dags
      - ./src:/usr/local/airflow/src
    ports:
      - "8080:8080"
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://user:password@redshift:5439/mydatabase
      - AIRFLOW__WEBSERVER__SECRET_KEY=your_secret_key

  kafka:
    build:
      context: ./kafka
    ports:
      - "9092:9092"
    environment:
      - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092
      - KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT
      - KAFKA_LISTENERS=PLAINTEXT://0.0.0.0:9092
      - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181

  zookeeper:
    image: wurstmeister/zookeeper:3.4.6
    ports:
      - "2181:2181"

  pyspark:
    build:
      context: ./pyspark
    volumes:
      - ./src:/usr/local/pyspark/src
    depends_on:
      - kafka

  redshift:
    image: amazon/redshift:latest
    ports:
      - "5439:5439"
    environment:
      - POSTGRES_USER=user
      - POSTGRES_PASSWORD=password
      - POSTGRES_DB=mydatabase

networks:
  default:
    external:
      name: my_network

volumes:
  airflow_data:
  kafka_data:
  pyspark_data: